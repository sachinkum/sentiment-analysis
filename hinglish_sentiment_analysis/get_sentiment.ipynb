{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hk4Op0R5pNqu"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "khM-uJspuUVN",
    "outputId": "681bd9f1-4d39-4768-9fc3-efd02fa3dd22"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtAee3k0pPC3"
   },
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "0Ad9Z584pUBU",
    "outputId": "ffd347b5-bf18-45fe-f338-4fd09001915f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "      <th>transliteration_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hajagiree</td>\n",
       "      <td>हजगिरी</td>\n",
       "      <td>hajagiree~हजगिरी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chekaanv</td>\n",
       "      <td>चेकॉव</td>\n",
       "      <td>chekaanv~चेकॉव</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spinagaarn</td>\n",
       "      <td>स्पिनगार्न</td>\n",
       "      <td>spinagaarn~स्पिनगार्न</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>medal</td>\n",
       "      <td>मेडल</td>\n",
       "      <td>medal~मेडल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chetthinaad</td>\n",
       "      <td>चेत्तिनाद</td>\n",
       "      <td>chetthinaad~चेत्तिनाद</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       english       hindi   transliteration_data\n",
       "0    hajagiree      हजगिरी       hajagiree~हजगिरी\n",
       "1     chekaanv       चेकॉव         chekaanv~चेकॉव\n",
       "2   spinagaarn  स्पिनगार्न  spinagaarn~स्पिनगार्न\n",
       "3        medal        मेडल             medal~मेडल\n",
       "4  chetthinaad   चेत्तिनाद  chetthinaad~चेत्तिनाद"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"transliteration_data.csv\", encoding=\"utf-8\")\n",
    "df[\"transliteration_data\"] = df.apply(lambda x: '~'.join([str(x.english), str(x.hindi)]), axis=1)\n",
    "lines = df[\"transliteration_data\"].tolist()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Bu90pYcnVtC"
   },
   "source": [
    "Data Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "immzSl8gnYqE"
   },
   "outputs": [],
   "source": [
    "english_words = []\n",
    "hindi_words = []\n",
    "english_chars = set()\n",
    "hindi_chars = set()\n",
    "total_samples = len(lines)\n",
    "\n",
    "# Process time in words and numerals\n",
    "for line in range(total_samples):\n",
    "    english_line = str(lines[line]).split('~')[0]\n",
    "\n",
    "    # Append '\\t' for start of the entity and '\\n' to signify end of the entity\n",
    "    hindi_line = '\\t' + str(lines[line]).split('~')[1] + '\\n'\n",
    "    english_words.append(english_line)\n",
    "    hindi_words.append(hindi_line)\n",
    "\n",
    "    for ch in english_line:\n",
    "        if ch not in english_chars:\n",
    "            english_chars.add(ch)\n",
    "\n",
    "    for ch in hindi_line:\n",
    "        if ch not in hindi_chars:\n",
    "            hindi_chars.add(ch)\n",
    "\n",
    "hindi_chars = sorted(list(hindi_chars))\n",
    "english_chars = sorted(list(english_chars))\n",
    "\n",
    "# dictionary to index each time in words character - key is index and value is time in words character\n",
    "english_words_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get time in words character given its index - key is time in words character and value is index\n",
    "english_words_char_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(english_chars):\n",
    "    english_words_index_to_char_dict[k] = v\n",
    "    english_words_char_to_index_dict[v] = k\n",
    "\n",
    "# dictionary to index each numerals character - key is index and value is numerals character\n",
    "hindi_words_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get numerals character given its index - key is numerals character and value is index\n",
    "hindi_words_char_to_index_dict = {}\n",
    "for k, v in enumerate(hindi_chars):\n",
    "    hindi_words_index_to_char_dict[k] = v\n",
    "    hindi_words_char_to_index_dict[v] = k\n",
    "\n",
    "max_len_english_words = max([len(line) for line in english_words])\n",
    "max_len_hindi_words = max([len(line) for line in hindi_words])\n",
    "\n",
    "tokenized_english_words = np.zeros(shape=(total_samples, max_len_english_words, len(english_chars)), dtype='float32')\n",
    "tokenized_hindi_words = np.zeros(shape=(total_samples, max_len_hindi_words, len(hindi_chars)), dtype='float32')\n",
    "target_data = np.zeros((total_samples, max_len_hindi_words, len(hindi_chars)), dtype='float32')\n",
    "\n",
    "# Vectorize the time in words and numerals\n",
    "\n",
    "for i in range(total_samples):\n",
    "    for k, ch in enumerate(english_words[i]):\n",
    "        tokenized_english_words[i, k, english_words_char_to_index_dict[ch]] = 1\n",
    "\n",
    "    for k, ch in enumerate(hindi_words[i]):\n",
    "        tokenized_hindi_words[i, k, hindi_words_char_to_index_dict[ch]] = 1\n",
    "\n",
    "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "        if k > 0:\n",
    "            target_data[i, k - 1, hindi_words_char_to_index_dict[ch]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VncFFjBgnQUV"
   },
   "source": [
    "Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "GNKnmiLvni5h",
    "outputId": "0c322ab9-e0b5-4ace-efbc-1fa865061edf"
   },
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None, len(english_chars)))\n",
    "encoder_LSTM = LSTM(256, return_state=True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n",
    "encoder_states = [encoder_h, encoder_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNm9JVRAoWuN"
   },
   "source": [
    "Inference modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqiLQDWQoQZm"
   },
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_input = Input(shape=(None, len(hindi_chars)))\n",
    "decoder_LSTM = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h, decoder_c]\n",
    "decoder_dense = Dense(len(hindi_chars), activation='softmax')\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLvXjdy27uAS"
   },
   "outputs": [],
   "source": [
    "def decoder(input_seq):\n",
    "    # Initial states value is coming from the encoder\n",
    "    states_val = encoder_model_inf.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, len(hindi_chars)))\n",
    "    target_seq[0, 0, hindi_words_char_to_index_dict['\\t']] = 1\n",
    "\n",
    "    translated_ent = ''\n",
    "    stop_condition = False\n",
    "\n",
    "    while not stop_condition:\n",
    "\n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "        max_val_index = np.argmax(decoder_out[0, -1, :])\n",
    "        sampled_numerals_char = hindi_words_index_to_char_dict[max_val_index]\n",
    "        translated_ent += sampled_numerals_char\n",
    "\n",
    "        if (sampled_numerals_char == '\\n') or (len(translated_ent) > max_len_hindi_words):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, len(hindi_chars)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "\n",
    "        states_val = [decoder_h, decoder_c]\n",
    "\n",
    "    return translated_ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_inf.load_weights(\"hinglish_hindi/encoder_inference_weight.h5\")\n",
    "decoder_model_inf.load_weights(\"hinglish_hindi/decoder_inference_weight.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "KZIfaDZxtyRC",
    "outputId": "94ade756-cc56-4dea-9eb0-31b3dbd4d838",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def transliterate(inpt):\n",
    "    trans = []\n",
    "    for x in inpt.split(' '):\n",
    "        array = np.zeros(shape=(max_len_english_words, len(english_chars)), dtype='float32')\n",
    "        for k, ch in enumerate(x):\n",
    "            array[k, english_words_char_to_index_dict[ch]] = 1\n",
    "        array = array[np.newaxis, ...]\n",
    "        translated_ent = decoder(array)\n",
    "        trans.append(translated_ent.strip())\n",
    "    return ' '.join(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'चलो चलें'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate('chalo chalein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ft_model = fasttext.load_model('/home/pretrained_models/cc.hi.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {0:'negative', 2:'positive', 1:'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = tf.keras.models.load_model('/home/apna_time/2020-02-13_19_58_01.496519')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(s):\n",
    "    s = transliterate(s)\n",
    "    s = np.array(ft_model.get_sentence_vector(s)[np.newaxis, ...])\n",
    "    return res_dict[np.argmax(sent_model.predict(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment('bohot accha hai')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transliteration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
